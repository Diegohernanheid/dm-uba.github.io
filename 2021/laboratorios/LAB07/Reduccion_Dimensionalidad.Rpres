Preprocesamiento
========================================================
autosize: true
width: 1200
height: 800

Reducción de Dimensionalidad
<br />
<br />
<br />
<br />
Santiago Banchero
<br />
Juan Manuel Fernández
<br />
Eloísa Píccoli
<br />
<br />

Minería de Datos - UBA


Contenidos
========================================================

Vamos a ver algunos tips para implementar las siguientes técnicas en R:
- Reducción de dimensionalidad
  + Low Variance Factor
  + Reducing Highly Correlated Columns
  + Variables Importantes (Random Forest)

Usaremos el dataset de Iris
```{r}
# Excluimos la columna que contiene la clase
# Nos aseguramos de que no haya faltantes

data = na.omit(iris[,-c(5)])

```


Reducción de dimensionalidad: Low Variance Factor
========================================================


```{r}
# Copiamos el dataframe para no pisarlo
lvf = data

# Normalizamos los datos (Min-Max) a un rango 0-1
for(i in 1:ncol(lvf)) {
  lvf[,i] <- (lvf[,i]-min(lvf[,i]))/(max(lvf[,i])-min(lvf[,i]))
  }

# Calculamos la varianza para cada atributo, redondeamos a 4 decimales, y ordenamos de menor a mayor
varianzas<-sort(round(apply(lvf, 2, var),4))

```

```{r echo=FALSE}
print(varianzas)
```

Reducción dimensionalidad: Atrib. Correlacionados
========================================================
Primero debemos analizar si hay candidatos, podemos hacerlo gráficamente con un heatmap:
```{r eval=FALSE}
library(gplots)
library(RColorBrewer)

# Matriz de Correlacion
ds.cor=cor(data) 

# Excluyo el triangulo superior de la matriz
ds.cor[upper.tri(ds.cor)] <- NA

heatmap.2(ds.cor,
          cellnote = round(ds.cor,1), 
          notecol="black", 
          main = "Correlación",
          trace="none",        
          margins =c(5,5),    
          col=brewer.pal('RdYlBu', n=5),  
          dendrogram="none",
          symm= T,
          Rowv=F)            
```

Reducción dimensionalidad: Atrib. Correlacionados
========================================================
El gráfico de heatmap presenta información sobre la correlación entre las variables, con colores de referencia:
<center>
<small>
```{r echo=FALSE}
library(gplots)
library(RColorBrewer)


# Matriz de Correlacion
ds.cor=cor(data) 

# Excluyo el triangulo superior de la matriz
ds.cor[upper.tri(ds.cor)] <- NA

heatmap.2(ds.cor,
          cellnote = round(ds.cor,1), 
          notecol="black", 
          main = "Correlación",
          trace="none",        
          margins =c(5,5),    
          col=brewer.pal('RdYlBu', n=5),  
          dendrogram="none",
          symm= T,
          Rowv=F)            
          
```
</small>
</center>

Atributos Correlacionados (++)
========================================================
Vamos a hacer el análisis "a mano" y con la librería "caret":
```{r}
# Calculo matriz de correlacion
matriz.correlacion<-round(cor(data),2)

# Excluyo triangulo superior para mayor claridad
matriz.correlacion[upper.tri(matriz.correlacion)] <- NA


print(matriz.correlacion)
```

Atributos Correlacionados (+++)
========================================================
Ahora lo hacemos con la librería Caret:
```{r}
library(caret)

# Buscamos atributos con correlaciones superiores a 0.75
highlyCorrelated <- findCorrelation(matriz.correlacion, cutoff=0.75)

# Imprimimos los nombres de los atributos que cumplen con la condición anterior
print(names(data.numeric[,highlyCorrelated]))

```
Luego deberíamos analizar eliminar esos atributos.


Reducción de dimensionalidad: Random Forests
========================================================
Utilizamos la técnica de Random Forest para ponderar los atributos en función de la importancia en la clasificación.
Primero corremos el modelo:
```{r}
library(randomForest)

model_rf<-randomForest(Species ~ ., data=iris, na.action = na.omit)
importance(model_rf)
```

Reducción de dimensionalidad: Random Forests (++)
========================================================
También podemos realizar una gráfica con la importancia de cada variable:
<center>
```{r}
varImpPlot(model_rf)
```
</center>
