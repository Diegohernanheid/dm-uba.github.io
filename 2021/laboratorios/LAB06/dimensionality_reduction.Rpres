Preprocesamiento
========================================================
autosize: true
width: 1200
height: 800

Reducción de Dimensionalidad
<br />
<br />
<br />
<br />
Santiago Banchero
<br />
Juan Manuel Fernández
<br />
Eloísa Píccoli
<br />
<br />

Minería de Datos - UBA


Contenidos
========================================================

Vamos a ver algunos tips para implementar las siguientes técnicas en R:
- Reducción de dimensionalidad
  + Low Variance Factor
  + Atributos altamente correlacionados
  + Variables Importantes (Random Forest)

Usaremos el dataset de Iris
```{r}
# Excluimos la columna que contiene la clase
# Nos aseguramos de que no haya faltantes

data = na.omit(iris[,-c(5)])

```


Low Variance Factor
========================================================


```{r}
# Copiamos el dataframe para no pisarlo
lvf = data

# Normalizamos los datos (Min-Max) a un rango 0-1
# Otra alternativa es usar scales::rescale sobre vectores

for(i in 1:ncol(lvf)) {
  lvf[,i] <- (lvf[,i]-min(lvf[,i]))/(max(lvf[,i])-min(lvf[,i]))
  }

# Calculamos la varianza para cada atributo, redondeamos a 4 decimales, y ordenamos de menor a mayor
varianzas<-sort(round(apply(lvf, 2, var),4))

```

```{r echo=FALSE}
print(varianzas)
```

Atributos Altamente Correlacionados (+)
========================================================
Primero debemos analizar si hay candidatos, podemos hacerlo gráficamente con un heatmap:
```{r eval=FALSE}
library(gplots)
# Matriz de Correlacion
ds.cor=cor(data) 
# Excluyo el triangulo superior de la matriz
ds.cor[upper.tri(ds.cor)] <- NA
heatmap.2(abs(ds.cor), # tomo los valores absolutos
          cellnote = round(ds.cor,2), 
          notecol="black", 
          main = "Correlación",
          trace="none",        
          margins =c(11,11),  
          col=terrain.colors(10,rev = TRUE),  
          dendrogram="none",
          symm= T, 
          Rowv=F,# Ordena la diagonal (en vez de dendograma)    
          breaks=c(0.7, 0.8,0.85, 0.99, 1)) # Corte para colores
                   
```

Atributos Altamente Correlacionados (++)
========================================================
El gráfico de heatmap presenta información sobre la correlación entre las variables, con colores de referencia:
<center>
<small>
```{r echo=FALSE}
library(gplots)
library(ggplot2)
# Matriz de Correlación
ds.cor=cor(data) 
# Excluyo el triangulo superior de la matriz
ds.cor[upper.tri(ds.cor)] <- NA

#breaks <- c(0.7, 0.8,0.85, 0.99, 1)

heatmap.2(abs(ds.cor),
          cellnote = round(ds.cor,2), 
          notecol="black", 
          main = "Correlación",
          trace="none",        
          margins =c(11,11),  
          col=terrain.colors(4,rev = FALSE),  
          dendrogram="none",
          symm= T, 
          Rowv=F,
          breaks=c(0.7, 0.8,0.85, 0.99, 1))

          
```
</small>
</center>

Atributos Altamente Correlacionados (+++)
========================================================
Vamos a hacer el análisis "a mano":
```{r}
# Calculo matriz de correlación
matriz.correlacion<-round(cor(data),2)

# Excluyo triangulo superior para mayor claridad
matriz.correlacion[upper.tri(matriz.correlacion)] <- NA


print(matriz.correlacion)
```

Atributos Altamente Correlacionados (+++)
========================================================
Ahora lo hacemos con la librería Caret:
```{r}
library(caret)

# Buscamos atributos con correlaciones superiores a 0.75
highlyCorrelated <- findCorrelation(cor(data), cutoff=0.75)

# Imprimimos los nombres de los atributos que cumplen con la condición anterior
print(names(data[,highlyCorrelated]))

```
Luego deberíamos analizar si eliminamos estos atributos.


Importancia de Atributos con Random Forest 
========================================================
Pondera cuán importantes son los atributos en la clasificación de la clase.

<center>
```{r}
library(randomForest)

# Ajustamos un modelo en función a las especies
model_rf<-randomForest(Species ~ ., data=iris, importance=TRUE)

# Observamos la importancia de los atributos
knitr::kable(round(importance(model_rf),2),"pipe")
```
<center/>
Importancia de Atributos con Random Forest (+)
========================================================

MeanDecreaseAccuracy muestra cuánta precisión pierde en promedio el modelo al eliminar la variable y el MeanDecreaseGini muestra cuánto contribuye la variable a la pureza del nodo:
<center>
```{r, out.width="30%"}
varImpPlot(model_rf)
```
</center>
